{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup:\n",
    "```\n",
    "conda create -n earth2studio python=3.12 -y \n",
    "conda activate earth2studio\n",
    "pip install uv\n",
    "export UV_CACHE_DIR=\"/projectnb/eb-general/wade/uv_cache\"\n",
    "uv pip install \"earth2studio @ git+https://github.com/NVIDIA/earth2studio.git@0.10.0\"\n",
    "uv pip install \"earth2studio[fcn]\"\n",
    "uv pip install numpy matplotlib pandas xarray cartopy cmocean tqdm\n",
    "uv pip install \"makani @ git+https://github.com/NVIDIA/modulus-makani.git@28f38e3e929ed1303476518552c64673bbd6f722\"\n",
    "uv pip install earth2studio[sfno]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference with SFNO checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/eb-general/wade/.conda/envs/earth2studio/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/projectnb/eb-general/wade/.conda/envs/earth2studio/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from earth2studio.io import ZarrBackend\n",
    "\n",
    "# from earth2studio.run import deterministic\n",
    "# from earth2studio.models.px import SFNO\n",
    "from deterministic_update import deterministic\n",
    "from SFNO_update import SFNO\n",
    "\n",
    "import earth2studio.data as data\n",
    "from earth2studio.models.auto import Package\n",
    "from utils import filename_to_year, datetime_range, open_hdf5 # these aren't used in this script currently\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import xarray as xr\n",
    "from typing import List\n",
    "import shutil\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earth2studio version: 0.10.0rc0\n",
      "makani version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "# print versions of makani and earth2studio\n",
    "import earth2studio\n",
    "print(f\"earth2studio version: {earth2studio.__version__}\")\n",
    "import makani\n",
    "print(f\"makani version: {makani.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? False\n",
      "CUDA is not available. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "\n",
    "    # Get the ID of the current GPU\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"Current GPU ID: {current_gpu}\")\n",
    "\n",
    "    # Get the name of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "    print(f\"Current GPU Name: {gpu_name}\")\n",
    "\n",
    "    print(f\"Memory (VRAM):      {torch.cuda.get_device_properties(current_gpu).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# CONFIG FOR INFERENCE RUN #############\n",
    "\n",
    "start_datetime = \"2019-09-03T00:00:00\" #\"2022-09-24T00:00:00\" # \"2021_09_20T00:00:00\" # \n",
    "variables_to_select = ['msl', 'u10m', 'v10m'] #['tcwv'] #Only save selected variables - it slows down inference SIGNIFICANTLY to save all 74 variables\n",
    "experiment_number = 0 # which experiment directory to output to\n",
    "n_steps = 4  # number of 6hr steps to forecast\n",
    "epochs_to_run = [20] # List or array of epochs/checkpoint numbers to run inference on\n",
    "\n",
    "# boring =  False\n",
    "ema = False\n",
    "\n",
    "# Create the inference name based on the start datetime and number of steps\n",
    "inference_name = datetime.fromisoformat(start_datetime).strftime(\"%Y_%m_%dT%H\")+'_nsteps'+str(n_steps)\n",
    "data_create_fp = \"/projectnb/eb-general/wade/sfno/inference_runs/Ian/Initialize_data/Initialize_\"+inference_name+\".nc\"\n",
    "\n",
    "# Calculate the final datetime based from the start datetime and number of steps\n",
    "final_datetime = (datetime.fromisoformat(start_datetime) + timedelta(hours = int(n_steps*6))).isoformat() \n",
    "\n",
    "# Directories\n",
    "results_out_dir = f\"/projectnb/eb-general/wade/sfno/inference_runs/sandbox/Experiment{str(experiment_number)}/{final_datetime[:10].replace('-', '_')}/\"\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/projectnb/eb-general/wade/sfno/inference_runs/Ian/Initialize_data/Initialize_2019_09_03T00_nsteps4.nc',\n",
       " '2019-09-04T00:00:00',\n",
       " '/projectnb/eb-general/wade/sfno/inference_runs/sandbox/Experiment0/2019_09_04/')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_create_fp, final_datetime, results_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already preprocessed: /projectnb/eb-general/wade/sfno/inference_runs/Ian/Initialize_data/Initialize_2019_09_03T00_nsteps4.nc\n",
      "Data loaded in 4765.38 seconds\n",
      "Loading model from /projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/ with checkpoint ckpt_mp0_epoch20.tar\n",
      "\u001b[32m2025-12-18 14:05:07.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdeterministic_update\u001b[0m:\u001b[36mdeterministic\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mRunning simple workflow!\u001b[0m\n",
      "\u001b[32m2025-12-18 14:05:07.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdeterministic_update\u001b[0m:\u001b[36mdeterministic\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mInference device: cpu\u001b[0m\n",
      "\u001b[32m2025-12-18 14:05:08.067\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdeterministic_update\u001b[0m:\u001b[36mdeterministic\u001b[0m:\u001b[36m90\u001b[0m - \u001b[32m\u001b[1mFetched data from DataArrayFile\u001b[0m\n",
      "\u001b[32m2025-12-18 14:05:08.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdeterministic_update\u001b[0m:\u001b[36mdeterministic\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mInference starting!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 5/5 [08:22<00:00, 100.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-18 14:13:30.442\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdeterministic_update\u001b[0m:\u001b[36mdeterministic\u001b[0m:\u001b[36m144\u001b[0m - \u001b[32m\u001b[1mInference complete\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">/</span>\n",
       "├── <span style=\"font-weight: bold\">d2m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">lat</span> (721,) float64\n",
       "├── <span style=\"font-weight: bold\">lead_time</span> (5,) timedelta64\n",
       "├── <span style=\"font-weight: bold\">lon</span> (1440,) float64\n",
       "├── <span style=\"font-weight: bold\">msl</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q100</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q1000</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q150</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q200</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q250</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q300</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q400</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q50</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q500</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q600</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q700</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q850</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">q925</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">sp</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t100</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t1000</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t150</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t200</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t250</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t2m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t300</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t400</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t50</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t500</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t600</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t700</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t850</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">t925</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">tcwv</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">time</span> (1,) datetime64\n",
       "├── <span style=\"font-weight: bold\">u100</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u1000</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u100m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u10m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u150</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u200</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u250</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u300</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u400</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u50</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u500</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u600</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u700</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u850</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">u925</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v100</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v1000</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v100m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v10m</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v150</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v200</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v250</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v300</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v400</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v50</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v500</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v600</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v700</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v850</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">v925</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z100</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z1000</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z150</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z200</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z250</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z300</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z400</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z50</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z500</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z600</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z700</span> (1, 5, 721, 1440) float32\n",
       "├── <span style=\"font-weight: bold\">z850</span> (1, 5, 721, 1440) float32\n",
       "└── <span style=\"font-weight: bold\">z925</span> (1, 5, 721, 1440) float32\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m/\u001b[0m\n",
       "├── \u001b[1md2m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mlat\u001b[0m (721,) float64\n",
       "├── \u001b[1mlead_time\u001b[0m (5,) timedelta64\n",
       "├── \u001b[1mlon\u001b[0m (1440,) float64\n",
       "├── \u001b[1mmsl\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq100\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq1000\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq150\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq200\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq250\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq300\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq400\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq50\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq500\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq600\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq700\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq850\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mq925\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1msp\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt100\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt1000\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt150\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt200\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt250\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt2m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt300\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt400\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt50\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt500\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt600\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt700\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt850\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mt925\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mtcwv\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mtime\u001b[0m (1,) datetime64\n",
       "├── \u001b[1mu100\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu1000\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu100m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu10m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu150\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu200\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu250\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu300\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu400\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu50\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu500\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu600\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu700\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu850\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mu925\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv100\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv1000\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv100m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv10m\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv150\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv200\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv250\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv300\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv400\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv50\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv500\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv600\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv700\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv850\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mv925\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz100\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz1000\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz150\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz200\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz250\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz300\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz400\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz50\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz500\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz600\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz700\u001b[0m (1, 5, 721, 1440) float32\n",
       "├── \u001b[1mz850\u001b[0m (1, 5, 721, 1440) float32\n",
       "└── \u001b[1mz925\u001b[0m (1, 5, 721, 1440) float32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset times: ['2019-09-03T00:00:00.000000000']\n",
      "Dataset dimensions: {'time': 1, 'lead_time': 5, 'lat': 721, 'lon': 1440}\n",
      "Lead times [ 0  6 12 18 24]\n",
      "Results saved to /projectnb/eb-general/wade/sfno/inference_runs/sandbox/Experiment0/2019_09_04/Checkpoint20_2019_09_03T00_nsteps4.nc\n",
      "Epoch 20 done: 545.60 seconds\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(data_create_fp):\n",
    "    print(f\"Data already preprocessed: {data_create_fp}\")\n",
    "else:\n",
    "    sys.exit(f\"Data not found use Create_Initial_Data.ipynb to create: {data_create_fp}\")\n",
    "\n",
    "# make this xarray into a dataarray file for earth2studio\n",
    "initial_data = data.DataArrayFile(data_create_fp)\n",
    "\n",
    "fine_tuning_start_epoch = 71 # the epoch where fine-tuning starts (important for correctly accessing the checkpoints)\n",
    "\n",
    "time_1 = time.time()\n",
    "print(f\"Data loaded in {time_1 - time_start:.2f} seconds\")\n",
    "\n",
    "for n_epoch in epochs_to_run: #[70]: #[1,10,20,30,40,50,60,70,80,90]: #np.arange(1,91,5): #70,1):\n",
    "    time_2 = time.time()\n",
    "\n",
    "    if ema:\n",
    "        results_out_fp = results_out_dir+f\"EMA_Checkpoint{n_epoch}_{inference_name}.nc\"\n",
    "    else:\n",
    "        results_out_fp =  results_out_dir+\"Checkpoint\"+str(n_epoch)+\"_\"+inference_name+'.nc' \n",
    "    \n",
    "    # Check if the results file already exists\n",
    "    if os.path.exists(results_out_fp):\n",
    "        print(f\"Results file {results_out_fp} already exists. Skipping to next epoch.\")\n",
    "        continue  # Skip the rest of the loop and go to the next iteration\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(results_out_fp), exist_ok=True)\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        if n_epoch < fine_tuning_start_epoch: # pre-fine-tuning phase epochs are numbered 1-70\n",
    "            src_dir = \"/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/\"\n",
    "            checkpoint_name = 'ckpt_mp0_epoch'+str(n_epoch)+'.tar'\n",
    "        else:\n",
    "            src_dir = \"/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/multistep_sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999-multistep2/\"\n",
    "            n_epoch_multistep2 = n_epoch - (fine_tuning_start_epoch - 1) # fine-tuning phase epochs are numbered from 1-20\n",
    "            checkpoint_name = 'ckpt_mp0_epoch'+str(n_epoch_multistep2)+'.tar'\n",
    "\n",
    "        print(f\"Loading model from {src_dir} with checkpoint {checkpoint_name}\")\n",
    "        # Load the model package from storage\n",
    "        model_package = Package(src_dir, cache = False)\n",
    "        model = SFNO.load_model(model_package, \n",
    "        checkpoint_name = checkpoint_name, EMA = ema\n",
    "        )\n",
    "\n",
    "        # Create the IO handler, store in memory\n",
    "        io = ZarrBackend()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # run inference\n",
    "            io = deterministic([start_datetime], n_steps, model, initial_data, io, \n",
    "            variables_list=variables_to_select\n",
    "            )\n",
    "\n",
    "        print(io.root.tree())\n",
    "\n",
    "        # save results to netcdf\n",
    "        # Open the Zarr group from the in-memory store using xarray\n",
    "        ds = xr.open_zarr(io.root.store)\n",
    "        \n",
    "        # SANITY CHECKING...\n",
    "        print(\"Dataset times:\", ds[\"time\"].values)\n",
    "        print(\"Dataset dimensions:\", {dim: ds.dims[dim] for dim in ds.dims})\n",
    "        print(\"Lead times\", ds[\"lead_time\"].values)\n",
    "\n",
    "        # Convert the 'time' coordinate in ds to datetime64 format\n",
    "        ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "        # Convert lead_time from nanoseconds to timedelta64[ns]\n",
    "        base_time = ds[\"time\"].values  # shape (n_time,)\n",
    "        lead_timedelta = ds[\"lead_time\"].values.astype(\"timedelta64[ns]\")  # shape (n_lead_time,)\n",
    "        # Broadcast to 2D: (time, lead_time)\n",
    "        valid_timesteps = (base_time[:, None] + lead_timedelta[None, :]).flatten() \n",
    "        # Drop the old lead_time coordinate\n",
    "        ds = ds.drop_vars(\"lead_time\")\n",
    "\n",
    "        # Assume ds has dimensions (time, lead_time, lat, lon) and only one time\n",
    "        initial_time = str(ds[\"time\"].values[0])  # Save the initial time as a string\n",
    "        # Remove the time dimension by selecting the first (and only) time\n",
    "        ds = ds.isel(time=0).drop_vars(\"time\")\n",
    "        # Add the initial time as a global attribute\n",
    "        ds.attrs[\"initial_time\"] = initial_time\n",
    "\n",
    "        # Create valid_time by adding lead_timedelta to base_time\n",
    "        ds = ds.rename({\"lead_time\": \"valid_time\"})\n",
    "        # Assign valid_time as a coordinate\n",
    "        ds = ds.assign_coords(valid_time=((\"valid_time\",), valid_timesteps))\n",
    "\n",
    "        # only save the final time step\n",
    "        if np.datetime64(final_datetime) in ds[\"valid_time\"].values:\n",
    "            ds = ds.sel(valid_time=[final_datetime])\n",
    "            ds = ds[variables_to_select]\n",
    "            ds.to_netcdf(results_out_fp, mode=\"w\", format=\"NETCDF4\")\n",
    "            print(f\"Results saved to {results_out_fp}\")\n",
    "        else:\n",
    "            print(f\"ERROR: final_datetime {final_datetime} not found in ds['valid_time']. No file saved.\")\n",
    "\n",
    "\n",
    "        #some cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        del model_package\n",
    "        del model\n",
    "        del io\n",
    "        del ds\n",
    "        gc.collect()\n",
    "        time_3 = time.time()\n",
    "        print(f\"Epoch {n_epoch} done: {time_3 - time_2:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 12MB\n",
      "Dimensions:     (valid_time: 1, lat: 721, lon: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 8B 2022-09-29\n",
      "  * lat         (lat) float64 6kB 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n",
      "  * lon         (lon) float64 12kB 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n",
      "Data variables:\n",
      "    msl         (valid_time, lat, lon) float32 4MB ...\n",
      "    u10m        (valid_time, lat, lon) float32 4MB ...\n",
      "    v10m        (valid_time, lat, lon) float32 4MB ...\n",
      "Attributes:\n",
      "    initial_time:  2022-09-24T00:00:00.000000000\n",
      "<xarray.Dataset> Size: 12MB\n",
      "Dimensions:     (valid_time: 1, lat: 721, lon: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 8B 2022-09-29\n",
      "  * lat         (lat) float64 6kB 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n",
      "  * lon         (lon) float64 12kB 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n",
      "Data variables:\n",
      "    msl         (valid_time, lat, lon) float32 4MB ...\n",
      "    u10m        (valid_time, lat, lon) float32 4MB ...\n",
      "    v10m        (valid_time, lat, lon) float32 4MB ...\n",
      "Attributes:\n",
      "    initial_time:  2022-09-24T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "# Compare the output of my forecast to Becca's saved forecast\n",
    "my_forecast ='/projectnb/eb-general/wade/sfno/inference_runs/sandbox/Experiment0/2022_09_29/Checkpoint70_2022_09_24T00_nsteps20.nc' \n",
    "beccas_forecast = '/projectnb/eb-general/wade/sfno/inference_runs/Ian/leadtime_fivedays/Checkpoint70_2022_09_24T00_nsteps20.nc'\n",
    "my_ds = xr.open_dataset(my_forecast)\n",
    "beccas_ds = xr.open_dataset(beccas_forecast)\n",
    "print(my_ds)\n",
    "print(beccas_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what epoch a checkpoint is (e.g. if epoch number is not in the filepath):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/training_checkpoints/best_ckpt_mp0.tar\n",
      "epoch: 70\n",
      "\n",
      "/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/training_checkpoints/ckpt_mp0.tar\n",
      "epoch: 70\n",
      "\n",
      "/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/multistep_sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999-multistep2/training_checkpoints/best_ckpt_mp0.tar\n",
      "epoch: 19\n",
      "\n",
      "/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/multistep_sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999-multistep2/training_checkpoints/ckpt_mp0.tar\n",
      "epoch: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "dir='/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/training_checkpoints/' # step 1 of training (epochs 1-70)\n",
    "dir2='/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/multistep_sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999-multistep2/training_checkpoints/' # step 2 of training (epochs 71-90\n",
    "\n",
    "# List of files to check\n",
    "files_to_check = [dir + \"best_ckpt_mp0.tar\", \n",
    "                  dir + \"ckpt_mp0.tar\",\n",
    "                  dir2 +  \"best_ckpt_mp0.tar\",\n",
    "                    dir2 + \"ckpt_mp0.tar\"\n",
    "                    ]\n",
    "                \n",
    "for filename in files_to_check:\n",
    "    # Load the checkpoint\n",
    "    # map_location='cpu' allows you to inspect this even without a GPU\n",
    "    # weights_only=False allows loading the full dictionary structure\n",
    "    checkpoint = torch.load(filename, map_location='cpu', weights_only=False)\n",
    "    epoch = checkpoint.get('epoch', 'N/A')\n",
    "    \n",
    "    print(f\"{filename:<25}\")\n",
    "    print(f'epoch: {str(epoch)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above output shows that the best_ckpt is the best ckpt *for each phase*, and ckpt_mp0 is the *final* ckpt for that phase\n",
    "- checkpoint numbering resets for each phase so phase 2 is numbered 1-20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from earth2studio.io import ZarrBackend\n",
    "from SFNO_update import SFNO\n",
    "import earth2studio.data as data\n",
    "from earth2studio.models.auto import Package\n",
    "from utils import filename_to_year, datetime_range, open_hdf5\n",
    "from deterministic_update import deterministic\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import xarray as xr\n",
    "from typing import List\n",
    "import shutil\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "\n",
    "    # Get the ID of the current GPU\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"Current GPU ID: {current_gpu}\")\n",
    "\n",
    "    # Get the name of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "    print(f\"Current GPU Name: {gpu_name}\")\n",
    "\n",
    "    print(f\"Memory (VRAM):      {torch.cuda.get_device_properties(current_gpu).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "############# Double check these before running the script #############\n",
    "#select start datetime and n_steps, each n_step = 6hrs\n",
    "start_datetime = \"2021-09-20T00:00:00\" # \"2021_09_20T00:00:00\"\n",
    "variables_to_select = ['tcwv'] #Only save selected variables - it slows down inference SIGNIFICANTLY to save all 74 variables\n",
    "experiment_number = 0\n",
    "n_steps = 20  #number of 6hr steps to forecast\n",
    "\n",
    "boring = False\n",
    "ema = False\n",
    "\n",
    "# Create the inference name based on the start datetime and number of steps\n",
    "inference_name = datetime.fromisoformat(start_datetime).strftime(\"%Y_%m_%dT%H\")+'_nsteps'+str(n_steps)\n",
    "data_create_fp = \"/projectnb/eb-general/wade/sfno/inference_runs/Ian/Initialize_data/Initialize_\"+inference_name+\".nc\"\n",
    "\n",
    "# Calculate the final datetime based from the start datetime and number of steps\n",
    "final_datetime = (datetime.fromisoformat(start_datetime) + timedelta(hours = int(n_steps*6))).isoformat() \n",
    "\n",
    "# Directories\n",
    "results_out_dir = f\"/projectnb/eb-general/wade/sfno/inference_runs/sandbox/Experiment{str(experiment_number)}/{final_datetime[:10].replace('-', '_')}/\"\n",
    "\n",
    "############# Double check these before running the script #############\n",
    "\n",
    "\n",
    "if os.path.exists(data_create_fp):\n",
    "    print(f\"Data already preprocessed: {data_create_fp}\")\n",
    "else:\n",
    "    sys.exit(f\"Data not found use Create_Initial_Data.ipynb to create: {data_create_fp}\")\n",
    "\n",
    "#make this xarray into a dataarray file for earth2studio\n",
    "initial_data = data.DataArrayFile(data_create_fp)\n",
    "\n",
    "time_1 = time.time()\n",
    "print(f\"Data loaded in {time_1 - time_start:.2f} seconds\")\n",
    "\n",
    "\n",
    "for n_epoch in np.arange(1,3): #70,1):\n",
    "    time_2 = time.time()\n",
    "    # if boring:\n",
    "    #     # Create the final datetime string in the desired format\n",
    "    #     \n",
    "    # else:# Create the final datetime string in the desired format\n",
    "\n",
    "    if ema:\n",
    "        results_out_fp = results_out_dir+f\"EMA_Checkpoint{n_epoch}_{inference_name}.nc\"\n",
    "    else:\n",
    "        results_out_fp = results_out_fp = results_out_dir+\"/Checkpoint\"+str(n_epoch)+\"_\"+inference_name+'.nc' \n",
    "    \n",
    "    # Check if the results file already exists\n",
    "    if os.path.exists(results_out_fp):\n",
    "        print(f\"Results file {results_out_fp} already exists. Skipping to next epoch.\")\n",
    "        continue  # Skip the rest of the loop and go to the next iteration\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(results_out_fp), exist_ok=True)\n",
    "\n",
    "        load_dotenv()  \n",
    "\n",
    "        # Make temporary folder with all the metadata in it.\n",
    "        src_dir = \"/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/\"\n",
    "        # Load the model package from storage\n",
    "        model_package = Package(src_dir, cache = False)\n",
    "        model = SFNO.load_model(model_package, checkpoint_name = 'ckpt_mp0_epoch'+str(n_epoch)+'.tar', EMA = ema)\n",
    "\n",
    "        # Create the IO handler, store in memory\n",
    "        io = ZarrBackend()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # run inference\n",
    "            io = deterministic([start_datetime], n_steps, model, initial_data, io, variables_list=variables_to_select)\n",
    "\n",
    "        print(io.root.tree())\n",
    "\n",
    "\n",
    "        # save results to netcdf\n",
    "        # Open the Zarr group from the in-memory store using xarray\n",
    "        ds = xr.open_zarr(io.root.store)\n",
    "\n",
    "        # Convert the 'time' coordinate in ds to datetime64 format\n",
    "        ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "        # Convert lead_time from nanoseconds to timedelta64[ns]\n",
    "        base_time = ds[\"time\"].values  # shape (n_time,)\n",
    "        lead_timedelta = ds[\"lead_time\"].values.astype(\"timedelta64[ns]\")  # shape (n_lead_time,)\n",
    "        # Broadcast to 2D: (time, lead_time)\n",
    "        valid_timesteps = (base_time[:, None] + lead_timedelta[None, :]).flatten() \n",
    "        # Drop the old lead_time coordinate\n",
    "        ds = ds.drop_vars(\"lead_time\")\n",
    "\n",
    "        # Assume ds has dimensions (time, lead_time, lat, lon) and only one time\n",
    "        initial_time = str(ds[\"time\"].values[0])  # Save the initial time as a string\n",
    "        # Remove the time dimension by selecting the first (and only) time\n",
    "        ds = ds.isel(time=0).drop_vars(\"time\")\n",
    "        # Add the initial time as a global attribute\n",
    "        ds.attrs[\"initial_time\"] = initial_time\n",
    "\n",
    "        # Create valid_time by adding lead_timedelta to base_time\n",
    "        ds = ds.rename({\"lead_time\": \"valid_time\"})\n",
    "        # Assign valid_time as a coordinate\n",
    "        ds = ds.assign_coords(valid_time=((\"valid_time\",), valid_timesteps))\n",
    "\n",
    "        # only save the final time step\n",
    "        if np.datetime64(final_datetime) in ds[\"valid_time\"].values:\n",
    "            ds = ds.sel(valid_time=[final_datetime])\n",
    "            ds = ds[variables_to_select]\n",
    "            ds.to_netcdf(results_out_fp, mode=\"w\", format=\"NETCDF4\")\n",
    "            print(f\"Results saved to {results_out_fp}\")\n",
    "        else:\n",
    "            print(f\"ERROR: final_datetime {final_datetime} not found in ds['valid_time']. No file saved.\")\n",
    "\n",
    "\n",
    "        #some cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        del model_package\n",
    "        del model\n",
    "        del io\n",
    "        del ds\n",
    "        gc.collect()\n",
    "        time_3 = time.time()\n",
    "        print(f\"Epoch {n_epoch} done: {time_3 - time_2:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "#     for n_epoch in np.arange(36,71,1):\n",
    "#         time_2 = time.time()\n",
    "#         if boring:\n",
    "#             # Create the final datetime string in the desired format\n",
    "#             results_out_fp = \"/barnes-engr-scratch2/C837824079/Experiment\"+str(experiment_number)+\"/Forecasts_Boring/\"+final_datetime[:10].replace(\"-\", \"_\")+\"/Checkpoint\"+str(n_epoch)+\"_\"+inference_name+'.nc'\n",
    "#         else:# Create the final datetime string in the desired format\n",
    "#             if ema:\n",
    "#                 results_out_fp = f\"/barnes-engr-scratch2/C837824079/Experiment{str(experiment_number)}/Forecast/EMA_9/Checkpoint{n_epoch}_{inference_name}.nc\"         \n",
    "#             else:\n",
    "#                 results_out_fp = \"/projectnb/eb-general/rbaiman/SFNO/Example_Inference/Example_Forecast/Checkpoint\"+str(n_epoch)+\"_\"+inference_name+'.nc'\n",
    "\n",
    "#         # Check if the results file already exists\n",
    "#         if os.path.exists(results_out_fp):\n",
    "#             print(f\"Results file {results_out_fp} already exists. Skipping to next epoch.\")\n",
    "#             continue  # Skip the rest of the loop and go to the next iteration\n",
    "#         else:\n",
    "#             os.makedirs(os.path.dirname(results_out_fp), exist_ok=True)\n",
    "\n",
    "#             load_dotenv()  \n",
    "\n",
    "#             # Make temporary folder with all the metadata in it.\n",
    "#             src_dir = \"/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999/\"\n",
    "\n",
    "#             # Load the model package from storage\n",
    "#             model_package = Package(src_dir, cache = False)\n",
    "#             model = SFNO.load_model(model_package, checkpoint_name = 'ckpt_mp0_epoch'+str(n_epoch)+'.tar', EMA = ema)\n",
    "\n",
    "#             # Create the IO handler, store in memory\n",
    "#             io = ZarrBackend()\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 # run inference\n",
    "#                 io = deterministic([start_datetime], n_steps, model, initial_data, io, variables_list=variables_to_select)\n",
    "\n",
    "#             print(io.root.tree())\n",
    "\n",
    "\n",
    "#             # save results to netcdf\n",
    "#             # Open the Zarr group from the in-memory store using xarray\n",
    "#             ds = xr.open_zarr(io.root.store)\n",
    "\n",
    "#             # Convert the 'time' coordinate in ds to datetime64 format\n",
    "#             ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "#             # Convert lead_time from nanoseconds to timedelta64[ns]\n",
    "#             base_time = ds[\"time\"].values  # shape (n_time,)\n",
    "#             lead_timedelta = ds[\"lead_time\"].values.astype(\"timedelta64[ns]\")  # shape (n_lead_time,)\n",
    "#             # Broadcast to 2D: (time, lead_time)\n",
    "#             valid_timesteps = (base_time[:, None] + lead_timedelta[None, :]).flatten() \n",
    "#             # Drop the old lead_time coordinate\n",
    "#             ds = ds.drop_vars(\"lead_time\")\n",
    "\n",
    "#             # Assume ds has dimensions (time, lead_time, lat, lon) and only one time\n",
    "#             initial_time = str(ds[\"time\"].values[0])  # Save the initial time as a string\n",
    "#             # Remove the time dimension by selecting the first (and only) time\n",
    "#             ds = ds.isel(time=0).drop_vars(\"time\")\n",
    "#             # Add the initial time as a global attribute\n",
    "#             ds.attrs[\"initial_time\"] = initial_time\n",
    "\n",
    "#             # Create valid_time by adding lead_timedelta to base_time\n",
    "#             ds = ds.rename({\"lead_time\": \"valid_time\"})\n",
    "#             # Assign valid_time as a coordinate\n",
    "#             ds = ds.assign_coords(valid_time=((\"valid_time\",), valid_timesteps))\n",
    "\n",
    "#             # only save the final time step\n",
    "#             if np.datetime64(final_datetime) in ds[\"valid_time\"].values:\n",
    "#                 ds = ds.sel(valid_time=[final_datetime])\n",
    "#                 ds = ds[variables_to_select]\n",
    "#                 ds.to_netcdf(results_out_fp, mode=\"w\", format=\"NETCDF4\")\n",
    "#                 print(f\"Results saved to {results_out_fp}\")\n",
    "#             else:\n",
    "#                 print(f\"ERROR: final_datetime {final_datetime} not found in ds['valid_time']. No file saved.\")\n",
    "\n",
    "\n",
    "#             #some cleanup\n",
    "#             torch.cuda.empty_cache()\n",
    "#             del model_package\n",
    "#             del model\n",
    "#             del io\n",
    "#             del ds\n",
    "#             gc.collect()\n",
    "#             time_3 = time.time()\n",
    "#             print(f\"Epoch {n_epoch} done: {time_3 - time_2:.2f} seconds\")\n",
    "\n",
    "\n",
    "# for n_epoch in np.arange(1,21,1):\n",
    "#     time_2 = time.time()\n",
    "#     # Create the final datetime string in the desired format\n",
    "#     if boring:\n",
    "#         # Create the final datetime string in the desired format\n",
    "#         results_out_fp = \"/barnes-engr-scratch2/C837824079/Experiment\"+str(experiment_number)+\"/Forecasts_Boring/\"+final_datetime[:10].replace(\"-\", \"_\")+\"/Checkpoint\"+str(n_epoch+70)+\"_\"+inference_name+'.nc'\n",
    "#     else:# Create the final datetime string in the desired format\n",
    "#         if ema:\n",
    "#             results_out_fp = f\"/barnes-engr-scratch2/C837824079/Experiment{str(experiment_number)}/Forecast/EMA_9/Checkpoint{n_epoch+70}_{inference_name}.nc\"\n",
    "#         else:\n",
    "#             results_out_fp = \"/projectnb/eb-general/rbaiman/SFNO/Example_Inference/Example_Forecast/Checkpoint\"+str(n_epoch+70)+\"_\"+inference_name+'.nc'\n",
    "\n",
    "    \n",
    "#     # Check if the results file already exists\n",
    "#     if os.path.exists(results_out_fp):\n",
    "#         print(f\"Results file {results_out_fp} already exists. Skipping to next epoch.\")\n",
    "#         continue  # Skip the rest of the loop and go to the next iteration\n",
    "#     else:\n",
    "#         os.makedirs(os.path.dirname(results_out_fp), exist_ok=True)\n",
    "\n",
    "#         load_dotenv()  \n",
    "\n",
    "#         # Make temporary folder with all the metadata in it.\n",
    "#         src_dir = \"/projectnb/eb-general/shared_data/data/processed/FourCastNet_sfno/Checkpoints_SFNO/multistep_sfno_linear_74chq_sc3_layers8_edim384_dt6h_wstgl2/v0.1.0-seed999-multistep2/\"\n",
    "\n",
    "#         # Load the model package from storage\n",
    "#         model_package = Package(src_dir, cache = False)\n",
    "#         model = SFNO.load_model(model_package, checkpoint_name = 'ckpt_mp0_epoch'+str(n_epoch)+'.tar', EMA = ema)\n",
    "\n",
    "#         # Create the IO handler, store in memory\n",
    "#         io = ZarrBackend()\n",
    "\n",
    "#         print(f\"Running inference for {inference_name}\")\n",
    "#         with torch.no_grad():\n",
    "#             # run inference\n",
    "#             io = deterministic([start_datetime], n_steps, model, initial_data, io, variables_list=variables_to_select)\n",
    "\n",
    "#         # print(io.root.tree())\n",
    "\n",
    "#         # save results to netcdf\n",
    "#         # Open the Zarr group from the in-memory store using xarray\n",
    "#         ds = xr.open_zarr(io.root.store)\n",
    "\n",
    "#         # Convert the 'time' coordinate in ds to datetime64 format\n",
    "#         ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "#         # Convert lead_time from nanoseconds to timedelta64[ns]\n",
    "#         base_time = ds[\"time\"].values  # shape (n_time,)\n",
    "#         lead_timedelta = ds[\"lead_time\"].values.astype(\"timedelta64[ns]\")  # shape (n_lead_time,)\n",
    "#         # Broadcast to 2D: (time, lead_time)\n",
    "#         valid_timesteps = (base_time[:, None] + lead_timedelta[None, :]).flatten() \n",
    "#         # Drop the old lead_time coordinate\n",
    "#         ds = ds.drop_vars(\"lead_time\")\n",
    "\n",
    "#         # Assume ds has dimensions (time, lead_time, lat, lon) and only one time\n",
    "#         initial_time = str(ds[\"time\"].values[0])  # Save the initial time as a string\n",
    "#         # Remove the time dimension by selecting the first (and only) time\n",
    "#         ds = ds.isel(time=0).drop_vars(\"time\")\n",
    "#         # Add the initial time as a global attribute\n",
    "#         ds.attrs[\"initial_time\"] = initial_time\n",
    "\n",
    "#         # Create valid_time by adding lead_timedelta to base_time\n",
    "#         ds = ds.rename({\"lead_time\": \"valid_time\"})\n",
    "#         # Assign valid_time as a coordinate\n",
    "#         ds = ds.assign_coords(valid_time=((\"valid_time\",), valid_timesteps))\n",
    "\n",
    "#         # only save the final time step\n",
    "#         if np.datetime64(final_datetime) in ds[\"valid_time\"].values:\n",
    "#             ds = ds.sel(valid_time=[final_datetime])\n",
    "#             ds = ds[variables_to_select]\n",
    "#             ds.to_netcdf(results_out_fp, mode=\"w\", format=\"NETCDF4\")\n",
    "#             print(f\"Results saved to {results_out_fp}\")\n",
    "#         else:\n",
    "#             print(f\"ERROR: final_datetime {final_datetime} not found in ds['valid_time']. No file saved.\")\n",
    "\n",
    "\n",
    "#         #some cleanup\n",
    "#         torch.cuda.empty_cache()\n",
    "#         del model_package\n",
    "#         del model\n",
    "#         del io\n",
    "#         del ds\n",
    "#         gc.collect()\n",
    "#         time_3 = time.time()\n",
    "#         print(f\"Epoch {n_epoch+70} done: {time_3 - time_2:.2f} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth2studio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
